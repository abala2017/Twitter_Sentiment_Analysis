{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the data\n",
    "data = pd.read_csv('training_data.csv', names = ['Sentiment', 'Id', 'Date', 'Flag', 'User', 'Text'])\n",
    "#Data was gotten from https://www.kaggle.com/kazanova/sentiment140\n",
    "del data['Id']\n",
    "del data['Date']\n",
    "del data['Flag']\n",
    "del data['User']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading in Embedding dims  dimensional\n",
    "embeddings = {}\n",
    "dims = 100\n",
    "with open(\"glove.twitter.27B/glove.twitter.27B.100d.txt\", encoding = \"utf8\")  as file:\n",
    "# Word Embeddings gotten from the twitter pre-trained vector at https://nlp.stanford.edu/projects/glove/\n",
    "    for line in file:\n",
    "        word, coefficients = line.split(maxsplit = 1)\n",
    "        coefficients = coefficients.split(\" \")\n",
    "        coefficients = np.array(coefficients, dtype=np.float32)\n",
    "        embeddings[word] = coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing function\n",
    "def preprocessTweet(X):\n",
    "    #lowercase\n",
    "    X = X.lower()\n",
    "    # separate @ and user\n",
    "    X = X.replace(\"@\", \" @ \")\n",
    "    # replace urls with \"url\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', X)\n",
    "    for url in urls:\n",
    "        X = X.replace(url, \"url\")\n",
    "    #replace multiple punctuation with single\n",
    "    repeatpunctuations = re.findall('[.,!?]{2,}',X)\n",
    "    for repeatpunctuation in repeatpunctuations:\n",
    "        X = X.replace(repeatpunctuation, repeatpunctuation[0])\n",
    "    #Emoji handling\n",
    "    smile = re.findall('[8;:=]['\"`\"\"\\\\\"'-][)d]',X)\n",
    "    lolface = re.findall('[8;:=]['\"`\"\"\\\\\"'-][p]',X)\n",
    "    sadface = re.findall('[8;:=]['\"`\"\"\\\\\"'-][(|/]',X)\n",
    "    neutralface = re.findall('[8;:=]['\"`\"\"\\\\\"'-][\\1]',X)\n",
    "    heart = re.findall('[<][3]',X)\n",
    "    for i in smile:\n",
    "        X = X.replace(i, \" smile \")\n",
    "    for i in lolface:\n",
    "        X = X.replace(i, \" lolface \")\n",
    "    for i in sadface:\n",
    "        X = X.replace(i, \" sadface \")\n",
    "    for i in neutralface:\n",
    "        X = X.replace(i, \" neutralface \")\n",
    "    for i in heart:\n",
    "        X = X.replace(i, \" heart \")\n",
    "    # number handling\n",
    "    numbers = re.findall('[0-9]{1,}',X)\n",
    "    for i in numbers:\n",
    "        X = X.replace(i, \" number \")\n",
    "    #remove contractions\n",
    "    contractions = re.findall(\"[']\",X)\n",
    "    for i in contractions:\n",
    "        X  = X.replace(i,\"\")\n",
    "    # add spaces between last word and punctuation\n",
    "    puncs = re.findall('[.!?,]',X)\n",
    "    for i in puncs:\n",
    "        X = X.replace(i,\" \"+i[0]+\" \")\n",
    "    # remove extended words ie 'wayyyyy' NEED to MAKE this part\n",
    "    # later find a way to determine whether the ending letters should be 1 or 2 letters ie hellll -> hell not hel\n",
    "    #extendedWords = re.findall('[a-z]{3,}',X)\n",
    "    #for i in extendedWords:\n",
    "        #X = X.replace(i, i[0])\n",
    "    #remove double spaces\n",
    "    X = re.sub(\"\\s\\s+\" , \" \", X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "data['Text'] = data['Text'].apply(preprocessTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1432532</th>\n",
       "      <td>4</td>\n",
       "      <td>in a dark stadium &amp;quot;painting&amp;quot; with jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291931</th>\n",
       "      <td>4</td>\n",
       "      <td>bird out though . now i must check all the dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606800</th>\n",
       "      <td>0</td>\n",
       "      <td>the guitar is still stuck in morsdorf . i am g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623532</th>\n",
       "      <td>0</td>\n",
       "      <td>@ bronxbebe number lol . nawww that was yeste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296942</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad to read about a number .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                               Text\n",
       "1432532          4  in a dark stadium &quot;painting&quot; with jo...\n",
       "1291931          4  bird out though . now i must check all the dis...\n",
       "606800           0  the guitar is still stuck in morsdorf . i am g...\n",
       "623532           0   @ bronxbebe number lol . nawww that was yeste...\n",
       "296942           0                 very sad to read about a number . "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffle the data\n",
    "shuffled_data = data.reindex(np.random.RandomState(seed=2020).permutation(data.index))\n",
    "shuffled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training, dev, and test\n",
    "X_data = shuffled_data['Text'].to_numpy()\n",
    "Y_data = shuffled_data['Sentiment'].to_numpy()\n",
    "#Convert Y to one-hot\n",
    "Y_data = Y_data/2\n",
    "Y_data = to_categorical(Y_data)\n",
    "#training: first 1.4 mil\n",
    "X_training = X_data[0:1400000]\n",
    "Y_training = Y_data[0:1400000]\n",
    "#dev: next 100 k\n",
    "X_dev = X_data[1400001:1500000]\n",
    "Y_dev = Y_data[1400001:1500000]\n",
    "#test:last 100 k\n",
    "X_test = X_data[150001:1600000]\n",
    "Y_test = Y_data[150001:1600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "#creates tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "#fits the input to the text, ie most common words being closer to 0 and more obscure being father away\n",
    "tokenizer.fit_on_texts(X_data) \n",
    "#converts the input to token indices\n",
    "X_training_tokens = tokenizer.texts_to_sequences(X_training)\n",
    "X_dev_tokens = tokenizer.texts_to_sequences(X_dev)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_training)\n",
    "#get largest list of words\n",
    "maxLen = max([len(s.split()) for s in X_data])\n",
    "#padding so all inputs are the same size\n",
    "X_train_pad = pad_sequences(X_training_tokens, maxlen = maxLen)\n",
    "X_dev_pad = pad_sequences(X_dev_tokens, maxlen = maxLen)\n",
    "X_train_pad = pad_sequences(X_test_tokens, maxlen = maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to make the embedding matrix\n",
    "#instantiate embedding matrix of zeroes\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, dims))\n",
    "#go through each word in the token list\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    #get the corresponding embedding vector (if it exists)\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    #check if its not none\n",
    "    if embedding_vector is not None:\n",
    "        #add that to the embedding matrix\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the model\n",
    "Model = Sequential()\n",
    "Model.add(\n",
    "    Embedding(\n",
    "        input_dim = len(tokenizer.word_index) + 1,\n",
    "        output_dim = dims,\n",
    "        weights = [embedding_matrix],\n",
    "        input_length = maxLen,\n",
    "        trainable = False\n",
    "    )\n",
    ")\n",
    "Model.add(\n",
    "    LSTM(\n",
    "        units = maxLen,\n",
    "        return_sequences = False\n",
    "        #possibly add dropout\n",
    "    )\n",
    ")\n",
    "Model.add(\n",
    "    Dense(\n",
    "        maxLen,\n",
    "        activation = 'relu'\n",
    "    )\n",
    ")\n",
    "Model.add(\n",
    "    Dense(\n",
    "        3,\n",
    "        activation = 'softmax'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 76, 100)           57388900  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 76)                53808     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 76)                5852      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 231       \n",
      "=================================================================\n",
      "Total params: 57,448,791\n",
      "Trainable params: 59,891\n",
      "Non-trainable params: 57,388,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(\n",
    "    optimizer = 'Adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1400000/1400000 [==============================] - 720s 515us/step - loss: 0.4741 - accuracy: 0.7718\n",
      "Epoch 2/10\n",
      "1400000/1400000 [==============================] - 746s 533us/step - loss: 0.4238 - accuracy: 0.8035\n",
      "Epoch 3/10\n",
      "1400000/1400000 [==============================] - 760s 543us/step - loss: 0.4099 - accuracy: 0.8117\n",
      "Epoch 4/10\n",
      "1400000/1400000 [==============================] - 743s 531us/step - loss: 0.4005 - accuracy: 0.8166\n",
      "Epoch 5/10\n",
      "1400000/1400000 [==============================] - 808s 577us/step - loss: 0.3943 - accuracy: 0.8203\n",
      "Epoch 6/10\n",
      "1400000/1400000 [==============================] - 809s 578us/step - loss: 0.3893 - accuracy: 0.8229\n",
      "Epoch 7/10\n",
      "1400000/1400000 [==============================] - 811s 579us/step - loss: 0.3849 - accuracy: 0.8253\n",
      "Epoch 8/10\n",
      "1400000/1400000 [==============================] - 842s 601us/step - loss: 0.3815 - accuracy: 0.8273\n",
      "Epoch 9/10\n",
      "1400000/1400000 [==============================] - 817s 584us/step - loss: 0.3778 - accuracy: 0.8294\n",
      "Epoch 10/10\n",
      "1400000/1400000 [==============================] - 813s 581us/step - loss: 0.3755 - accuracy: 0.8307\n"
     ]
    }
   ],
   "source": [
    "Training_Loss, Training_Accuracy = Model.fit(\n",
    "    x = X_train_pad,\n",
    "    y = Y_training,\n",
    "    batch_size = 2048,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999/99999 [==============================] - 15s 154us/step\n"
     ]
    }
   ],
   "source": [
    "Dev_Loss, Dev_Accuracy = Model.evaluate(\n",
    "    x = X_dev_pad,\n",
    "    y = Y_dev,\n",
    "    batch_size = 2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss : 0.3868369434377294\n",
      "Dev Accuracy : 0.824128270149231\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev Loss : \" + str(Dev_Loss))\n",
    "print(\"Dev Accuracy : \" + str(Dev_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
