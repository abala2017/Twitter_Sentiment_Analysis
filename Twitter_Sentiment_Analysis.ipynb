{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the data\n",
    "data = pd.read_csv('training_data.csv', names = ['Sentiment', 'Id', 'Date', 'Flag', 'User', 'Text'])\n",
    "#Data was gotten from https://www.kaggle.com/kazanova/sentiment140\n",
    "del data['Id']\n",
    "del data['Date']\n",
    "del data['Flag']\n",
    "del data['User']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading in Embedding dims  dimensional\n",
    "embeddings = {}\n",
    "dims = 100\n",
    "with open(\"glove.twitter.27B/glove.twitter.27B.100d.txt\", encoding = \"utf8\")  as file:\n",
    "# Word Embeddings gotten from the twitter pre-trained vector at https://nlp.stanford.edu/projects/glove/\n",
    "    for line in file:\n",
    "        word, coefficients = line.split(maxsplit = 1)\n",
    "        coefficients = coefficients.split(\" \")\n",
    "        coefficients = np.array(coefficients, dtype=np.float32)\n",
    "        embeddings[word] = coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing function\n",
    "def preprocessTweet(X):\n",
    "    #lowercase\n",
    "    X = X.lower()\n",
    "    # separate @ and user\n",
    "    X = X.replace(\"@\", \" @ \")\n",
    "    # replace urls with \"url\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', X)\n",
    "    for url in urls:\n",
    "        X = X.replace(url, \"url\")\n",
    "    #replace multiple punctuation with single\n",
    "    repeatpunctuations = re.findall('[.,!?]{2,}',X)\n",
    "    for repeatpunctuation in repeatpunctuations:\n",
    "        X = X.replace(repeatpunctuation, repeatpunctuation[0])\n",
    "    #Emoji handling\n",
    "    smile = re.findall('[8;:=]['\"`\"\"\\\\\"'-][)d]',X)\n",
    "    lolface = re.findall('[8;:=]['\"`\"\"\\\\\"'-][p]',X)\n",
    "    sadface = re.findall('[8;:=]['\"`\"\"\\\\\"'-][(|/]',X)\n",
    "    neutralface = re.findall('[8;:=]['\"`\"\"\\\\\"'-][\\1]',X)\n",
    "    heart = re.findall('[<][3]',X)\n",
    "    for i in smile:\n",
    "        X = X.replace(i, \" smile \")\n",
    "    for i in lolface:\n",
    "        X = X.replace(i, \" lolface \")\n",
    "    for i in sadface:\n",
    "        X = X.replace(i, \" sadface \")\n",
    "    for i in neutralface:\n",
    "        X = X.replace(i, \" neutralface \")\n",
    "    for i in heart:\n",
    "        X = X.replace(i, \" heart \")\n",
    "    # number handling\n",
    "    numbers = re.findall('[0-9]{1,}',X)\n",
    "    for i in numbers:\n",
    "        X = X.replace(i, \" number \")\n",
    "    #remove contractions\n",
    "    contractions = re.findall(\"[']\",X)\n",
    "    for i in contractions:\n",
    "        X  = X.replace(i,\"\")\n",
    "    # add spaces between last word and punctuation\n",
    "    puncs = re.findall('[.!?,]',X)\n",
    "    for i in puncs:\n",
    "        X = X.replace(i,\" \"+i[0]+\" \")\n",
    "    # remove extended words ie 'wayyyyy' NEED to MAKE this part\n",
    "    # later find a way to determine whether the ending letters should be 1 or 2 letters ie hellll -> hell not hel\n",
    "    #extendedWords = re.findall('[a-z]{3,}',X)\n",
    "    #for i in extendedWords:\n",
    "        #X = X.replace(i, i[0])\n",
    "    #remove double spaces\n",
    "    X = re.sub(\"\\s\\s+\" , \" \", X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "data['Text'] = data['Text'].apply(preprocessTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1432532</th>\n",
       "      <td>4</td>\n",
       "      <td>in a dark stadium &amp;quot;painting&amp;quot; with jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291931</th>\n",
       "      <td>4</td>\n",
       "      <td>bird out though . now i must check all the dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606800</th>\n",
       "      <td>0</td>\n",
       "      <td>the guitar is still stuck in morsdorf . i am g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623532</th>\n",
       "      <td>0</td>\n",
       "      <td>@ bronxbebe number lol . nawww that was yeste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296942</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad to read about a number .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                               Text\n",
       "1432532          4  in a dark stadium &quot;painting&quot; with jo...\n",
       "1291931          4  bird out though . now i must check all the dis...\n",
       "606800           0  the guitar is still stuck in morsdorf . i am g...\n",
       "623532           0   @ bronxbebe number lol . nawww that was yeste...\n",
       "296942           0                 very sad to read about a number . "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffle the data\n",
    "shuffled_data = data.reindex(np.random.RandomState(seed=2020).permutation(data.index))\n",
    "shuffled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training, dev, and test\n",
    "X_data = shuffled_data['Text'].to_numpy()\n",
    "Y_data = shuffled_data['Sentiment'].to_numpy()\n",
    "#Convert Y to one-hot\n",
    "Y_data = Y_data/2\n",
    "Y_data = to_categorical(Y_data)\n",
    "#training: first 1.4 mil\n",
    "X_training = X_data[0:1400000]\n",
    "Y_training = Y_data[0:1400000]\n",
    "#dev: next 100 k\n",
    "X_dev = X_data[1400001:1500000]\n",
    "Y_dev = Y_data[1400001:1500000]\n",
    "#test:last 100 k\n",
    "X_test = X_data[150001:1600000]\n",
    "Y_test = Y_data[150001:1600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "#creates tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "#fits the input to the text, ie most common words being closer to 0 and more obscure being father away\n",
    "tokenizer.fit_on_texts(X_data) \n",
    "#converts the input to token indices\n",
    "X_training_tokens = tokenizer.texts_to_sequences(X_training)\n",
    "X_dev_tokens = tokenizer.texts_to_sequences(X_dev)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_training)\n",
    "#get largest list of words\n",
    "maxLen = max([len(s.split()) for s in X_data])\n",
    "#padding so all inputs are the same size\n",
    "X_train_pad = pad_sequences(X_training_tokens, maxlen = maxLen)\n",
    "X_dev_pad = pad_sequences(X_dev_tokens, maxlen = maxLen)\n",
    "X_train_pad = pad_sequences(X_test_tokens, maxlen = maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to make the embedding matrix\n",
    "#instantiate embedding matrix of zeroes\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, dims))\n",
    "#go through each word in the token list\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    #get the corresponding embedding vector (if it exists)\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    #check if its not none\n",
    "    if embedding_vector is not None:\n",
    "        #add that to the embedding matrix\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the model\n",
    "Model = Sequential()\n",
    "Model.add(\n",
    "    Embedding(\n",
    "        input_dim = len(tokenizer.word_index) + 1,\n",
    "        output_dim = dims,\n",
    "        weights = [embedding_matrix],\n",
    "        input_length = maxLen,\n",
    "        trainable = False\n",
    "    )\n",
    ")\n",
    "Model.add(\n",
    "    LSTM(\n",
    "        units = maxLen,\n",
    "        return_sequences = True\n",
    "        #possibly add dropout\n",
    "    )\n",
    ")\n",
    "Model.add(\n",
    "    LSTM(#\n",
    "        units = maxLen,\n",
    "        return_sequences = False\n",
    "    )\n",
    ")\n",
    "Model.add(\n",
    "    Dense(\n",
    "        maxLen,\n",
    "        activation = 'relu'\n",
    "    )\n",
    ")\n",
    "Model.add(\n",
    "    Dense(\n",
    "        3,\n",
    "        activation = 'softmax'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 76, 100)           57388900  \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 76, 76)            53808     \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 76)                46512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 76)                5852      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 231       \n",
      "=================================================================\n",
      "Total params: 57,495,303\n",
      "Trainable params: 106,403\n",
      "Non-trainable params: 57,388,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(\n",
    "    optimizer = 'Adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1400000/1400000 [==============================] - 2265s 2ms/step - loss: 0.4733 - accuracy: 0.7712\n",
      "Epoch 2/10\n",
      "1400000/1400000 [==============================] - 2545s 2ms/step - loss: 0.4178 - accuracy: 0.8067\n",
      "Epoch 3/10\n",
      "1400000/1400000 [==============================] - 2538s 2ms/step - loss: 0.4037 - accuracy: 0.8146\n",
      "Epoch 4/10\n",
      "1400000/1400000 [==============================] - 2551s 2ms/step - loss: 0.3944 - accuracy: 0.8197\n",
      "Epoch 5/10\n",
      "1400000/1400000 [==============================] - 2545s 2ms/step - loss: 0.3881 - accuracy: 0.8234\n",
      "Epoch 6/10\n",
      "1400000/1400000 [==============================] - 2535s 2ms/step - loss: 0.3831 - accuracy: 0.8262\n",
      "Epoch 7/10\n",
      "1400000/1400000 [==============================] - 2528s 2ms/step - loss: 0.3785 - accuracy: 0.8289\n",
      "Epoch 8/10\n",
      "1400000/1400000 [==============================] - 2624s 2ms/step - loss: 0.3749 - accuracy: 0.8305\n",
      "Epoch 9/10\n",
      "1400000/1400000 [==============================] - 2662s 2ms/step - loss: 0.3713 - accuracy: 0.8325\n",
      "Epoch 10/10\n",
      "1400000/1400000 [==============================] - 2661s 2ms/step - loss: 0.3687 - accuracy: 0.8339\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable History object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-42e0abfc8674>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m Training_Loss, Training_Accuracy = Model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_pad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable History object"
     ]
    }
   ],
   "source": [
    "Training_Loss = Model.fit(\n",
    "    x = X_train_pad,\n",
    "    y = Y_training,\n",
    "    batch_size = 2048,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999/99999 [==============================] - 49s 495us/step\n"
     ]
    }
   ],
   "source": [
    "Dev_Loss, Dev_Accuracy = Model.evaluate(\n",
    "    x = X_dev_pad,\n",
    "    y = Y_dev,\n",
    "    batch_size = 2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss : 0.3789831615569364\n",
      "Dev Accuracy : 0.8293383121490479\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev Loss : \" + str(Dev_Loss))\n",
    "print(\"Dev Accuracy : \" + str(Dev_Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
